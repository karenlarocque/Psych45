Section 10.30.2013 - Interactions, Contrasts, and Mediation
========================================================
## Homework Question A 

What question are we trying to answer?

*Do family-friendly programs in organizations (e.g., flexible work hours, on-site childcare, etc.) have an effect on employee satisfaction?*

Let's clear out our working space, load in the data, etc.
```{r workspace}
rm(list=ls())

d<-read.csv("http://www.stanford.edu/class/psych252/data/families.csv")
```

Taking a look at the data.

```{r data}
str(d)
```

Now we know that we have:
`N = 68` companies in our sample

**Measures**

`famprog:` the amount of family-friendly programs from (1) Nothing at all to (9) Amazing family-friendliness

`empsatis:` the average rating of employee satisfaction from (1) Extremely unsatisfied to (7) Extremely satisfied

`perfam:` the percentage of employees with families in the organization from 0% to 100%

**(a) Describe the data**

Note the plots in your homework! The plot on the right can only be created if you factor the variables. (We'll do this later, because we probably would want to keep these variables as continuous when working with them.) The coplot you can create using continuous variables!

```{r coplot}
with(d, coplot(empsatis ~ famprog | perfam))
```

Let's also take a moment to check out our DV, employee satisfaction.

```{r}
hist(d$empsatis)
```

**(c) Main effects: Does the number of programs affect employee satisfaction? Is the percentage of families who use the programs correlated with employee satisfaction?**

We could run a simple regression where `famprog`, the number of family-friendly programs, predicts `empsatis`, employee satisfaction

```{r simple reg}
with(d, summary(lm(empsatis ~ famprog)))
```

Seems like the asnwer is yes, but we have a weak effect. Can we do something to describe the data better? Taking a look at our plot, what do we think might be going on?

We could use our other predictor `perfam`, and look for an interaction on employee satisfaction.

```{r interaction continuous}
with(d, summary(lm(empsatis ~ famprog*perfam)))
```

Now that we see the expected interaction how do we interpret the lower order effects? We will see how to get a decent interpretation later. For now we know that it can't possibly mean that there is a negative relationship between famprog and satisfaction because we saw the data and the model above. 

Let's try centering both of our variables and see if we can get a better intuition.

```{r}
with(d, summary(lm(empsatis ~ scale(famprog, scale = F)*scale(perfam, scale = F))))
```

Do the lower order coefficients conform to our intuitions better now?

Compare the p-value of `famprog` with that obtained without an interaction term.

**(c) Do family-friendly programs improve employee satisfaction overall?**

Answer: Yes, there is a marginal positive main effect of the number of family programs on employee satisfaction, and we would give the b, t, df, and p
Use df from residual SE at bottom of lm output
`b = .07, t(64) = 1.84, p = .07`

**(d) Does the percentage of employees with families impact the effect of family programs on employee satisfaction?**

Answer: There is a significant interaction between the number of family programs available and the percentage of families who use these programs,
`b = .007, t(64) = 2.06, p = .04`

**(e) Interpret the answer by examining the effect of family programs on employee satisfaction for companies who have the average number of employees with families**

Revisit our centered lm:

```{r} 
with(d, summary(lm(empsatis ~ scale(famprog, scale = F)*scale(perfam, scale = F))))
```

Because we've centered (at the mean), the simple effects reported here are at the mean

So for the number of family programs at the mean of the percentage of families who use the programs, `b = .065, p = .07`

So, what did we mean above by the main effect? Lesson: don't rely on these terms to make yourself understood. Be clear in describing your interpretation and exactly what it means. This is also helpful in coming up with a good mechanistic interpretation. 

**(e) Interpret the answer by examining the effect of family programs on employee satisfaction for companies at +1SD and -1SD of mean % who use programs**

Say we want to know about companies that have a lot of employees with families who use these programs (+1SD). We calculate this by subtracting 1SD from centered value.

```{r sd above}
with(d, summary(lm(empsatis ~ scale(famprog, scale = F)*I(scale(perfam, scale = F)-sd(perfam)))))
```

So for the number of family programs at +1SD above the mean of the percentage of families who use programs, the effect of family programs on employee satisfaction is `b = .14, p < .01.`

*Remember that we subtract one SD to describe the effect at one SD above the mean! You're subtracting these levels from your centered variable, so in the case of -1 SD, so +1 SD*

Now, for companies with few families who use these progrmas, we'll look at -1SD below the mean of the percentage of families who use these programs

```{r sd below}
with(d, summary(lm(empsatis ~ scale(famprog, scale = F)*I(scale(perfam, scale = F)+sd(perfam)))))
```

`b = -.01, ns`

Answer: List the simple effects at each level.

For companies where few families use the family programs, the number of programs does not affect employee satisfaction, `b = -.01, t(64) = -.23, p >.80.` However, for companies where a lot of families use the family programs, the number of family programs is associated with higher employee satisfaction, `b = .14, t(64) = 2.70, p = .007.`

**(f) What do you conclude? Write out the story. Remember to use the appropriate numbers to make the story as useful as possible!**

```{r descriptives}
mean(d$perfam)
mean(d$perfam)+sd(d$perfam)
mean(d$perfam)-sd(d$perfam)
```

## Contrasts

Let's say that we wanted to transform the predictors into categorical variables, using one or more categorical variables with different means on a quantitative variable. (*What kind of test is this?*)

There are a few different decisions we as researchers could make at this point. Do we want to create a categorical variable with two groups (*High and Low number of programs*) or multiple groups (*Low, Middle, and High number of programs*)? How should we split up our other variables of interest? These are examples of the subjective decisions you make as researchers!

To make two equal groups, we'll split the variable using the `median` function.  Then, we change it into a categorical variable using the `findInterval` function.  

```{r median}
quantfp2 = median(d$famprog); print(quantfp2)
d$famprogcat = findInterval(d$famprog, quantfp2)

table(d$famprogcat)

d$FPcat <- factor(d$famprogcat, labels=c('lowprog','highprog'))
table(d$FPcat)
```

Another quick way to do this would be to find the median and then use the `ifelse` function to create the new variable.

```{r}
median(d$famprog)
d$Fcat <- ifelse(d$famprog<6, c('highprog'), c('lowprog'))

table(d$Fcat)
```

To make three groups, we'll use the `quantile` function to divide our variable `perfam`, the percentage of employees with families in the orgnanization, into thirds (*note that we've specified our probabilities as .33 and .66 accordingly). Again we change it into a categorical variable using the `findInterval` function.  This will allow us to have a categorical variable with three equal groups. 

```{r quantile}
quantpf3 = quantile(d$perfam, probs = c(.34, .66)); print(quantpf3)   
d$perfamcat = findInterval(d$perfam, quantpf3)  		

table(d$perfamcat)

d$PerCat <- factor(d$perfamcat, labels=c('low use','middle use', 'high use'))
table(d$PerCat)
```

Here's another way to create multiple categories in your data!  We can specify our values calculated above, `r quantpf3`

```{r categorical option 2}
d$PCat[d$perfam<51.7] <- 'Low Use'
d$PCat[d$perfam>51.7 & d$perfam<59] <- 'Middle Use'
d$PCat[d$perfam>=59] <- 'High Use'

table(d$PCat)
```

```{r second look}
str(d) # note variables that are chr not factor! problem for contrasts
contrasts(d$Fcat)=cbind(-1,1)
```

Let's plot our data! 

```{r plotting}
with(d, 
     {interaction.plot(FPcat, PerCat, empsatis,
      xlab = 'Number of Programs',
      ylab = 'Employee Satisfaction',
      trace.label = 'Percentage Use'
                       ) })
```

How can we make bar graphs?  (Since we're using two categorical predictors)

We'll also add error bars! Since they're easier to eyeball (remember the paper Benoit mentioned in class!), we'll include the 95% confidence interval, though I'll give you a formula for the standard error that you could use too.

```{r formulas}
sem <- function(x) {sd(x) / sqrt(length(x))}

ci95 <- function(x) {sem(x) * 1.96}
```

```{r plotting error bars}
ms <- aggregate(empsatis ~ FPcat + PerCat, data=d, mean)
ms$errs <- aggregate(empsatis ~ FPcat + PerCat, data=d, ci95)$empsatis
print(ms)

library(ggplot2)
ggplot(ms, aes(x=FPcat, y=empsatis, fill=PerCat)) + 
  geom_bar(position=position_dodge(), stat="identity", colour="black", size=.3) + # Use black outlines
  geom_errorbar(aes(ymin = ms$empsatis-ms$errs, ymax=ms$empsatis+ms$errs), width=.2, position=position_dodge(width=.9)) +
  xlab("Number of Programs") +
  ylab("Employee Satisfaction") +
  theme_bw()
```

Let's revisit our data, but just take a look at the companies that had a low number of programs, since that seemed to be where interesting things were happening!

```{r subsetting data}
subset(d, FPcat=='lowprog') -> l
str(l)
```

What if we change our contrasts? What predictions might we make using the data?

```{r contrasts}
plot(l$PerCat, l$empsatis)
levels(l$PerCat)
contrasts(l$PerCat) = cbind(c(-1,-1,2), c(-1,1,0)) #which groups are these contrasts comparing?

with(l, summary(lm(empsatis ~ PerCat)))

```

We've talked in class about contrasts that are orthogonal. Why do we want to create orthogonal contrasts?

Let's create a formula to test whether contrasts are orthogonal (if we have three groups)!

```{r}
c_orth_3 <- function(x) {(x[1,1]*x[1,2])+(x[2,1]*x[2,2])+(x[3,1]*x[3,2])}
```

Let's create some orthogonal and non-orthogonal contrasts and see if our function works.

```{r}
c1 <- cbind(c(-2,1,1), c(0,1,-1)); c_orth_3(c1)
c2 <- cbind(c(-2,1,1), c(1,1,-2)); c_orth_3(c2)
c3 <- cbind(c(-2,1,1), c(1,1,-3)); c_orth_3(c3) #Remember contrasts must sum to 0!
```

Now let's see if we can create a function for other contrasts!

```{r}
c_orth_4 <- function(x) {
  a <- (x[1,1]*x[1,2])+(x[2,1]*x[2,2])+(x[3,1]*x[3,2])+(x[4,1]*x[4,2]);
  b <- (x[1,2]*x[1,3])+(x[2,2]*x[2,3])+(x[3,2]*x[3,3])+(x[4,2]*x[4,3]);
  c <- (x[1,1]*x[1,3])+(x[2,1]*x[2,3])+(x[3,1]*x[3,3])+(x[4,1]*x[4,3]);
  d <- cbind(a,b,c); rownames(d)=c('Contrasts'); print(d)
  e <- a+b+c; names(e)=c('Sum'); print(e) }
```


```{r}
c1 <- cbind(c(-3,1,1,1), c(0,0,1,-1), c(-1,1,1,-1)); c_orth_4(c1)
c2 <- cbind(c(1,1,-1,-1), c(1,-1,1,-1), c(-1,1,1,-1)); c_orth_4(c2)
```

Let's take a closer look at each of these contrasts to see what groups they would be comparing!

```{r}
print(c2)
```

## Question E - Mediation!

Let's clean up our screens and load in the next data set!

```{r}
rm(list=ls())

d<-read.csv("http://www.stanford.edu/class/psych252/data/caffeine.csv")
str(d)
```

**Measures**

What mediational question might we ask with these data?

**IV**: *Coffee* - 20 subjects in each group either had 0 cups, 2 cups, or 4 cups

**DV**: *Performance* - on a stats quiz with 10 problems, 5-89 points

**Possible Mediator 1**: *Number of problems attempted* (hyperactivity)

**Possible Mediator 2**: *Accuracy* - how likely they were to get a problem right if they tried (better success)

What should we do first?

```{r}
hist(d$perf)
table(d$coffee)
```

We should probably recode coffee cups into number of coffee cups!

```{r}
d$cups = 0*as.numeric(d$coffee==1) + 2*as.numeric(d$coffee==2) + 4*as.numeric(d$coffee==3) 
table(d$cups)
```

First question: Does the number of problems attempted (hyperactivity) mediate the effect of coffee on performance?

We need to run three models. There is one model that we never run (the effect of the mediator on the DV, without the IV included):

```{r}
with(d, summary(lm(perf~numprob)))
```

The first model we need to look at is the direct path, does coffee predict performance?  If not, we can abandon this whole endeavor!

```{r model 1}
problm1<-lm(perf~cups,data=d)
summary(problm1) # yes, it predicts, c=3.74
c<-problm1$coefficients[2] # We'll save our coefficients for our Sobel test later!

problm1<-lm(perf~coffee,data=d) # Note that we get the same results whether we recode coffee or not, just different coefficients
summary(problm1)
```

Now let's check out Model 2, whether the IV affects the mediator, in other words, does coffee predict the number of problems attempted?

```{r model 2}
problm2<-lm(numprob~cups,data=d)
summary(problm2) # yes, a=0.52
a<-summary(problm2)$coefficients[2,1]
s_a<-summary(problm2)$coefficients[2,2]
```

Our final model, Model 3, is the effect of coffee on performance mediated by the effect of the number of problems.

```{r model 3}
problm3<-lm(perf~cups+numprob,data=d)
summary(problm3) 

c_prime<-summary(problm3)$coefficients[2,1]
b<-summary(problm3)$coefficients[3,1]
s_b<-summary(problm3)$coefficients[3,2]
```

The direct effect of coffee (c) disappeared and the number of problems attempted (b) is significant. We could answer yes, there is mediation, but let's be more formal.

Let's perform  the conventional Sobel test, adding in the standard error of a and standard error of b.

```{r sobel}
s_ab <- sqrt(b^2*s_a^2+a^2*s_b^2+s_a^2*s_b^2)
s_ab # standard error of a*b
p_s_ab<-pnorm(a*b/s_ab,lower.tail=F)
p_s_ab # p of ratio of a*b over its s.e.
```

Now let's repeat the procedure for the second mediation analysis. The question now is: does accuracy meadiate the effect of coffee on performance? 

We did Model 1 above and have significant c (the direct path)

Now let's move on to Model 2, does coffee predict accuracy?

```{r second med}
accurm2<-lm(accur~cups,data=d)
summary(accurm2)
```

No, coffee consumption does not predict accuracy, so according to Baron & Kenny we can stop and conclude there is no mediation. But lets procede, using a=-0.00014.

```{r}
a2<-summary(accurm2)$coefficients[2,1]
s_a2<-summary(accurm2)$coefficients[2,2]
```

Now model 3, is the effect of coffee on performance mediated by the effect of accuracy?

```{r}
accurm3<-lm(perf~cups+accur,data=d)
summary(accurm3) # now the effect of coffee remains as well as an effect of accur.

b2<-summary(accurm3)$coefficients[3,1]
s_b2<-summary(accurm3)$coefficients[3,2]
```

Perform conventional sobel test, adding standard error of a and b.

```{r}
s_ab2 <- sqrt(b2^2*s_a2^2+a2^2*s_b2^2+s_a2^2*s_b2^2)
s_ab2 # standard error of a*b
p_s_ab2<-pnorm(a2*b2/s_ab2,lower.tail=F)
p_s_ab2 # p of ratio of a2*b2 over its s.e.
```

Conclusion: Coffee and accuracy both contribute to performance and in this case there is no mediation there. However, the effect of coffee is mediated by the number of problems attempted. 
