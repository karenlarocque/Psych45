Section Week 8 - Linear Mixed Models
========================================================
Much of the content adapted from **Winter, B. (2013). Linear models and linear mixed effects models in R with linguistic applications. arXiv:1308.5499.** [Link](http://arxiv.org/pdf/1308.5499.pdf)


How is a linear mixed effects model different from the linear models we know already?

Linear mixed models are a type of regression model that take into account variation that is not explained by the independent variables of interest in your study.

Let's say you're interested in language, and more specifically how voice pitch is related to politeness. You ask your subjects to respond to hypothetical scenarios that are either more formal situations that require politeness (e.g., giving an excuse for being late to a professor) or more informal situations (e.g., explaining to a friend why you're late), and measure their voice pitch. Each subject is given a list of scenarios, so each subject gives multiple polite or informal responses. You also take note of each of your subjects' genders, since you know that's another important influence on voice pitch.

In a linear model as we've seen so far, we would model this as:

`pitch ~ politeness + sex + ε`

Where the last term is our error term. This error term represents the deviations from our predictions due to “random” factors that we cannot control experimentally.

With this kind of data, since each subject gave multiple responses, we can immediately see that this would violate the independence assumption that's important in linear modeling: Multiple responses from the same subject cannot be regarded as independent from each other. Every person has a slightly different voice pitch, and this is going to be an idiosyncratic factor that  affects all responses from the same subject, thus rendering these different responses inter-dependent rather than independent.

## Random Effects

The way we’re going to deal with this situation is to add a random effect for subject. This allows us to resolve this non-independence by assuming a different “baseline” pitch value for each subject. So, subject 1 may have a mean voice pitch of 233 Hz across different utterances, and subject 2 may have a mean voice pitch of 210 Hz. In our model, we account through these individual differences in voice pitch using random effects for subjects.

We'll look at an example with some data borrowed from **Winter and Grawunder (2012)**:
```{r}
d = read.csv('http://www.bodowinter.com/tutorial/politeness_data.csv')

str(d)
summary(d)
head(d)

# Let's rename some things!
names(d)[names(d)=="attitude"] <- "condition"
names(d)[names(d)=="frequency"] <- "pitch"
names(d)

table(d$subject, d$gender)
```

Now let's visualize the data:
```{r}
library(ggplot2)
qplot(condition, pitch, facets = . ~ subject, colour = subject, geom = "boxplot", data = d) + theme_bw()
```

Subjects "F#" are female subjects. Subjects "M#" are male subjects. You immediately see that males have lower voices than females (as is to be expected). But on top of that, within the male and the female groups, you see lots of 
individual variation, with some people having relatively higher values for their sex and others having relatively lower values. 
 
We can model these individual differences by assuming different **random intercepts** for each subject. That is, each subject is assigned a different intercept value, and the mixed model estimates these intercepts for you.

Get an idea for the different subject means
```{r}
with(d, aggregate(pitch~subject, FUN='mean'))
```

And, there is within-subject correlation of pitches:
```{r}
pol_subj = subset(d, condition=='pol'); head(pol_subj)
inf_subj = subset(d, condition=='inf'); head(inf_subj)

qplot(pol_subj$pitch, inf_subj$pitch) + 
  geom_smooth(method="lm", fullrange=TRUE)
```

Now you begin to see why the mixed model is called a “mixed” model. The linear models that we considered so far have been “fixed-effects-only” models that had one or more fixed effects and a general error term “ε”. With the linear model, we essentially divided the world into things that we somehow understand or that are somehow systematic (the *fixed effects*, or the explanatory variables); and *random error*, things that we cannot control for or that we don’t understand (ε). But crucially, this latter part, the unsystematic part of the model, did not have any interesting structure. We simply had a general across-the-board error term. 

In the mixed model, we add one or more random effects to our fixed effects. These random effects essentially give structure to the error term “ε”. In the case of our model here, we add a random effect for “subject”, and this characterizes idiosyncratic variation that is due to individual differences. 

A *random effect* is generally something that can be expected to have a non-systematic, idiosyncratic, unpredictable, or "random" influence on your data. In experiments, that's often something like your subject or item, and you want to generalize over the idiosyncracies of individual subjects and items.

*Fixed effects*, on the other hand, are expected to have a systematic and predictable influence on your data.
 
The mixture of fixed and random effects is what makes the mixed model a "mixed model."

*What are some examples of fixed and random effects that you might see in mixed modeling?*

- **Fixed effects:** the independent variables that might normally be included in your analyses. For instance: gender, age, your study conditions

- **Random effects:** the variables that are specific to your data sample. For instance: speaker, word, listener, items (i.e., individual stimuli), scenario

## Random Intercepts

Turning back to our model, our old formula was:

`pitch ~ condition + gender + ε`

Our updated formula looks like this: 

`pitch ~ condition + gender + (1|subject) + ε`

“`(1|subject)`” is the R syntax for a *random intercept*. What this is saying is “assume an intercept that’s different for each subject” … and “1” stands for the intercept here. You can think of this formula as telling your model that it should expect that there’s going to be multiple responses per subject, and these responses will depend on each subject’s baseline level. This effectively resolves the non-independence that stems from having multiple responses by the same subject.

Note that the formula still contains a general error term “ε”. This is necessary because even if we accounted for individual by-subject variation, there’s still going to be “random” differences between different utterances from the same subject.

In the study we've been discussing, there’s an additional source of non-independence beyond subject differences in voice pitch that needs to be accounted for: Let's say we had different items. One item, for example, was an “asking for a favor” scenario. Here, subjects had to imagine asking a professor for a favor (polite condition), or asking a peer for a favor (informal condition). Another item was an “excusing for coming too late” scenario, which was similarly divided between polite and informal. In total, there were 7 such different items. 
 
Similar to the case of by-subject variation, we also expect by-item variation. For example, there might be something special about “excusing for coming too late” which leads to overall higher pitch (maybe because it’s more embarrassing than asking for a favor), regardless of the influence of politeness. And whatever it is that makes one item different from another, the responses of the different subjects in our experiment might similarly be affected by this random factor that is due to item-specific idiosyncrasies. That is, if “excusing for coming to late” leads to high pitch (for whatever reason), it’s going to do so for subject 1, subject 2, subject 3 and so on. Thus, the different responses to one item cannot be regarded as independent, or, in other words, there’s something similar to multiple responses to 
the same item – even if they come from different people. Again, if we did not account for these interdependencies, we would violate the independence assumption. 

We do this by adding an additional random effect: 
 
`pitch ~ condition + gender + (1|subject) + (1|scenario) + ε` 
 
```{r}
qplot(1, pitch, facets = condition ~ scenario, colour = scenario, geom = "boxplot", data = d) + theme_bw()
```
 
So, on top of *different intercepts for different subjects*, we now also have *different intercepts for different items*. We now “resolved” those non-independencies (our model knows that there are multiple responses per subject and per item), and we accounted for by-subject and by-item variation in overall pitch levels. 


First we need to load in the package for lmer, `lme4`:
```{r}
#install.packages('lme4')
library(lme4)
```

How can we find missing data and outliers?
```{r}
# How to find missing values?
which(is.na(d$pitch)==T)

# How about outliers?
bp <- with(d, boxplot(pitch ~ condition*gender, 
                      col=c("white","lightgray"),condition))

bp$out
subset(d, pitch==bp$out)
```

Let's start exploring some mixed models!
```{r}
lmer(pitch ~ condition, data=d) # this doesn't work! Need a random error term to use lmer

# model w/rfx
rs_subj_reml = lmer(pitch ~ condition + (1|subject), data=d)
rs_subj_ml = lmer(pitch ~ condition + (1|subject), REML=FALSE, data=d)

# model info
summary(rs_subj_ml)
summary(rs_subj_reml)
anova(rs_subj_reml)
coef(rs_subj_reml)
AIC(rs_subj_reml)
logLikelihood = logLik(rs_subj_reml)
deviance = -2*logLikelihood[1]; deviance

# compare to the data
qplot(condition, pitch, facets = . ~ subject, colour = subject, geom = "boxplot", data = d) + theme_bw()
```

Getting p-values
--------------------
```{r}
# how to get p-vals
#install.packages('languageR')
#library(languageR)
#rs.mcmc = pvals.fnc(rs_subj_reml, nsim = 10000, addPlot = T)
#print(rs.mcmc)

# Now approx p-val w/Kenward-Roger’s approximations
#install.packages('lmerTest')
library(lmerTest)
rs_subj_reml = lmer(pitch ~ condition + (1|subject), data=d) # you have to re-run lmer now!
anova(rs_subj_reml, ddf="Kenward-Roger")

# or, use model comparison!
rs_subj_ml = lmer(pitch ~ condition + (1|subject), REML=FALSE, data=d)
rs_null_ml = lmer(pitch ~ 1 + (1|subject), REML=FALSE, data=d)

anova(rs_null_ml, rs_subj_ml)
```
Here, the addition of the fixed factor "condition" significantly improved model fit, $\chi^2$ (1) = `8.74`, *p* < 0.01.


**Item effects** (random intercept for each "item/stimulus"):
Different stimuli may elicit different values of "pitch"; as a result, pitch for a given scenario may be correlated across subjects, and even within a subject for the polite and informal conditions. We can model this as a random effect!
```{r}
rs_subjscene_reml = lmer(pitch ~ condition + (1|subject) + (1|scenario), data=d)
summary(rs_subjscene_reml)
anova(rs_subjscene_reml)
coef(rs_subjscene_reml)
print(c(deviance = -2*logLik(rs_subjscene_reml)))
```

Let’s focus on the output for the random effects first: 

Have a look at the column standard deviation. This is a measure of the variability for each random effect that you added to the model. You can see that scenario (“item”) has much less variability than subject. Based on our boxplots from above, where we saw more idiosyncratic differences between subjects than between items, this is to be expected. Then, you see “Residual” which stands for the variability that’s not due to either scenario or subject. This is our “ε” again, the “random” deviations from the predicted values that are not due to subjects and items. Here, this reflects the fact that each and every utterance has some factors that affect pitch that are outside the scope of our experiment. 

What would this have looked like if we hadn't used a mixed model?

```{r}
summary(lm(pitch~condition,d))
```

Not only should we not do this because it violates the assumption of independence, but it obscures the pattern that we would otherwise see in our data.

If you look back at the boxplot that we constructed earlier, you can see that the value 202.588 Hz seems to fall halfway between males and females – and this is indeed what this intercept represents. It’s the average of our data for the informal condition.

As we didn’t inform our model that there’s two sexes in our dataset, the intercept is particularly off, in between the voice pitch of males and females.

Let's add gender as a fixed effect:

```{r}
rs_gen_subj_reml = lmer(pitch ~ condition + gender + (1|subject) + (1|scenario), data=d) 
summary(rs_gen_subj_reml)
```

Note that we added “gender” as a fixed effect because the relationship between sex and pitch is systematic and predictable (i.e., we expect females to have higher pitch). This is different from the random effects subject and item, where the relationship between these and pitch is much more unpredictable and “random.”

Note that compared to our earlier model without the fixed effect gender, the variation that’s associated with the random effect “subject” dropped considerably. This is because the variation that’s due to gender was confounded with the variation that’s due to subject. The model didn’t know about males and females, and so it’s predictions were relatively more off, creating relatively larger residuals. 

We see that males and females differ by about 109 Hz (at least for "informal"). And the intercept is now much higher (256.846 Hz), as it now represents the female category (for the informal condition). The coefficient for the effect of attitude didn’t change much. 

Now we can compare our models using ANOVA, to see if one accounts for significantly more variance than another.

```{r}
rs_gen_subjscene_ml = lmer(pitch ~ condition + gender + (1|subject) + 
                             (1|scenario), REML=FALSE, data=d) 

rs_null_subjscene_ml = lmer(pitch ~ gender + (1|subject) + (1|scenario), 
                            REML=FALSE, data=d) 

anova(rs_gen_subjscene_ml, rs_null_subjscene_ml)

chisq_val = -2*((logLik(rs_gen_subjscene_ml)[1]) - (logLik(rs_null_subjscene_ml)[1])); chisq_val
chisq_df = 6-5; chisq_df
pval = pchisq(chisq_val, chisq_df, lower.tail=TRUE); pval
```

This is known as a likelihood ratio test.

Now, you can summarize your results by saying something like, "Model comparison confirmed that politeness significantly predicts level of pitch, $\chi^2$ (1) = 11.62, p = 0.00065); specifically, polite scenarious result in a lowering of pitch by about 19.7 Hz ± 5.6 Hz (standard error), relative to informal scenarios.

You can also use the ANOVA function to look at the difference between additive and interactive models.

```{r}
rs_intergen_subjscene_ml = lmer(pitch ~ condition * gender + (1|subject) + (1|scenario), REML=FALSE, data=d) 
summary(rs_intergen_subjscene_ml)

anova(rs_gen_subjscene_ml, rs_intergen_subjscene_ml)
```
Here, we can see that adding the interaction doesn't significantly improve on the additive model; that is, the model with the interactive term doesn't significantly improve model fit. So, we'll stick with the simpler additive model.

## Random Slopes

```{r}
coef(rs_gen_subjscene_ml)
```

You see from our model coefficients for the model:

`politeness.model2 = lmer(pitch ~ condition + gender + (1|subject) + (1|scenario), REML=FALSE, data=d)`

that each scenario and each subject is assigned a different intercept. That’s what we would expect, given that we’ve told the model with “(1|subject)” and “(1|scenario)” to take by-subject and by-item variability into account. 
 
But note also that the fixed effects (condition and gender) are all the same for all subjects and items. Our model is what is called a random intercept model. In this model, we account for baseline-differences in pitch, but we assume that whatever the effect of politeness is, it’s going to be the same for all subjects and items. 
 
But is that a valid assumption? In fact, often times it’s not – it is quite expected that some items would elicit more or less politeness. That is, the effect of politeness might be different for different items. Likewise, the effect of politeness might be different for different subjects. For example, it might be expected that some people are more polite, others less. So, what we need is a random slope model, where subjects and items are not only allowed to have differing intercepts, but where they are also allowed to have different slopes for the effect of politeness (i.e., different effects of condition on pitch).

First, let's take a look at our data:
```{r}

(ggplot(d, aes(x=condition, y=pitch))
     #tell ggplot what data is, and x and y variables
     +facet_wrap(~subject,scales='free')
     #add a wrapping by unique combos of 2 variable
     #vary scales per facet.
     +geom_point()
     #add the points as representations
     +stat_smooth(method='lm',aes(group=1))
     #add the linear fits.
     )
```

Now, we'll run a linear mixed-effect model with random intercepts and slopes for subjects.
```{r}
politeness.model.rs = lmer(pitch ~ condition + gender + (1 + condition|subject) + (1 |scenario), REML=FALSE, data=d)
summary(politeness.model.rs)
```

Note that the only thing that we changed is the random effects, which now look a little more complicated. The notation “(1 + condition|subject)” means that you tell the model to expect differing baseline-levels of pitch (the intercept, represented by 1) as well as differing responses to the main factor in question, which is “condition” in this case. You then do the same for items, using the term "(1 + condition|scenario)."

Have a look at the coefficients of this updated model by typing in the following:
```{r}
coef(politeness.model.rs)
```

Now, the column with the by-subject and by-item coefficients for the effect of politeness (“conditionpol”) is different for each subject and item. Note, however, that it’s always negative and that many of the values are quite similar to each other. This means that despite individual variation, there is also consistency in how politeness affects the voice: for all of our speakers, the voice tends to go down when speaking politely, but for some people it goes down slightly more so than for others. 
 
Have a look at the column for gender. Here, the coefficients do no change. That is because we didn’t specify random slopes for the by-subject or by-item effect of gender.

Now we can see if this model with random slopes for subjects is significantly better than the model with just random intercepts.
```{r}
rs_gen_subjscene_con_reml = lmer(pitch ~ condition + gender + (1 + condition|subject) + 
                                   (1 + condition|scenario), REML=TRUE, data=d)

rs_gen_subjscene_reml = lmer(pitch ~ condition + gender + (1|subject) + 
                               (1|scenario), REML=TRUE, data=d) 

anova(rs_gen_subjscene_reml, rs_gen_subjscene_con_reml)
```
So, it appears that we don't need to include random slope for condition in the model; however, others would argue that we should keep our models maximal! To read more about that, check out this paper [Barr, Levy, Scheepers, & Tilly, 2013](http://idiom.ucsd.edu/~rlevy/papers/barr-etal-2013-jml.pdf).


Also, just as a note, `anova()` uses ML deviance, but you can calculate REML deviance by hand:
```{r}
rs_gen_subjscene_con_ml = lmer(pitch ~ condition + gender + (1 + condition|subject) + 
                                   (1 + condition|scenario), REML=FALSE, data=d)

rs_gen_subjscene_ml = lmer(pitch ~ condition + gender + (1|subject) + 
                               (1|scenario), REML=FALSE, data=d) 

anova(rs_gen_subjscene_ml, rs_gen_subjscene_con_ml)

# To use REML:
chisq_val = -2*((logLik(rs_gen_subjscene_reml)[1]) - (logLik(rs_gen_subjscene_con_reml)[1])); chisq_val
```



## Some final notes about mixed modeling

There are a few important things to say here: You might ask yourself “Which random slopes should I specify?” … or even “Are random slopes necessary at all?” 
 
Conceptually, it makes a lot of sense to include random slopes along with random intercepts. After all, you can almost always expect that people differ with how they react to an experimental manipulation! And likewise, you can almost always expect that the effect of an experimental manipulation is not going to be the same for all of items in your experiment. 
 
In the model above, our whole study crucially rested on stating something about politeness. We were not interested in gender differences, but they are well worth controlling for. This is why we had random slopes for the effect of attitude (by subjects and item) but not gender. In other words, we only modeled by-subject and by-item variability in how politeness affects pitch. 

We've talked a lot about the different assumptions of the linear model. The good news is: Everything that we discussed in the context of the linear model applies straightforwardly to mixed models. So, you also have to worry about collinearity and outliers. And you have to worry about homoscedasticity (*equality of variance*) and potentially about lack of normality.

Independence, being the most important assumption, requires a special word: One of the main reasons we moved to mixed models rather than just working with linear models was to resolve non-independencies in our data. However, mixed models can still violate independence … if you’re missing important fixed or random effects. So, for example, if we analyzed our data with a model that didn’t include the random effect “subject”, then our model would not “know” that there are multiple responses per subject. This amounts to a violation of the independence assumption. So choose your fixed effects and random effects carefully, and always try to resolve non-independencies. 

### Some other notes:
If your dependent variable is…
- **Continuous:** use a linear regression model with mixed effects
- **Binary:** use a logistic regression model with mixed effects

Function `lmer` is used to fit linear mixed models, function `glmer` is used to fit generalized (non-Gaussian) linear mixed models.

## Exploring our homework data!

Now we'll try working with the data from upcoming homework 5 (that was also used in the tutorial in Week 0 if you were there!) to get used to using mixed models.

Let's read about our data in `kv0.csv`!

Our study design here features both **between-subject** factors (2 attention conditions) and **within-subject** factors (# of possible solutions to a word task, solving anagrams). The dependent variable was score on a memory test (higher numbers reflect better performance). There were 10 study participants divided between the two conditions; they each completed three problems in each category of # of possible solutions (1, 2, or 3).

This is a *repeated measures design*.  

The question we want to answer is: **How does score depend on attention and number of possible solutions?**

Variables:

- **subidr**: Subject ID

- **attnr**: 1 = divided attention condition; 2 = focused attention condition

- **num1**: only one solution to the anagram

- **num2**: two possible solutions to the anagram

- **num3**: three possible solutions to the anagram

Let's read in our data!

```{r}
d0 <- read.csv('http://www.stanford.edu/class/psych252/data/kv0.csv')

str(d0)

# Make sure to factor subject!
d0$subidr = factor(d0$subidr)
```

Note that our data is in *wide* or *short-form*: `'data.frame':  20 obs. of  5 variables` 

By short-form, we mean that the within-subject observations are displayed in separate columns, and each subject occupies a single row. 

We need the data in *long-form* for `lmer`. The function `reshape` is an economic way to convert between wide and long formats.

```{r}
d1 <- reshape(d0, direction="long", idvar="subidr", varying=list(c("num1","num2","num3")), timevar="num", v.names="score" )

head(d1)
str(d1)
```

Now the data is in long form: `'data.frame':  60 obs. of  4 variables`

The number of observations that we have in long format is equal to the number of observations in wide format times the product of levels of the repeated measures (within) variables.

In this case we only have one withing subject variable with 3 levels (number of possible solutions = 1, 2, or 3), so 20 * 3 = 60 observations

The added variables are identifiers now.

We can also use the function `melt()` from the `reshape2` package to get our data into long form.

Our `id.vars` are those variables that we want to be the same for each subject, and the `measure.vars` are those that are repeated measures on each subject:

```{r reshape_d}
install.packages('reshape2')
library(reshape2)

dl <- melt(d0, id.vars = c("subidr", "attnr"), measure.vars = c("num1", "num2", "num3"))

head(dl) # note 'variable' and 'value' names not specified

colnames(dl) <- c("id", "attn", "num", "score")
head(dl)
str(dl)
```

Basically, we now have a long-form dataframe with 3 rows for each subject.

### Setting up variables

Since the levels of 'num' were created from the original column names (i.e., num1, num2, num3), R interprets the 'num' variable as a factor. However, we want to treat num as a quantitative variable, and need to force num to be numeric. We also want the subject id ('id') to be a factor:

```{r forcevars_dl}
dl$num <- as.numeric(dl$num)
dl$id <- factor(dl$id)
```

We also need to rescale `id` to 1:10 within each level of `attn`, since the subject id ('id') is 11:20 when `attn` is 'focused'. So we need to select only these values of 'id', and transform them to 1:10. This requires creating a new variable, cond.id, in the dataframe, dl.

```{r rescaleid_dl}
dl$subj.id <- as.numeric(dl$id)
dl$subj.id[dl$attn=="focused"] = dl$subj.id[dl$attn=="focused"] - 10
head(dl)
```

Now that we have our dataset ready to go, try plotting the data, and creating and comparing some mixed models using this data. 

There are some hints and suggested analyses at the bottom of this document, but try to explore on your own, create your own R script and then compare! Feel free to work with anyone sitting around you.

### Possible analyses

Start with a simple regression and random intercept for subject

```{r}
res1 = lmer(score ~ num + (1|subj.id), dl)
summary(res1)
```

The regression is singnificant and interpretable as usual

Regression equation:

$score$ = 4.76 + .6 * $num$

Let's visualize what we're modeling with the random intercept model.

```{r}
(ggplot(dl, aes(x=num, y=score))
     #tell ggplot what data is, and x and y variables
     +facet_wrap(~subj.id, ncol=5, scales='free')
     #add a wrapping by unique combos of 2 variable
     #set num columns, and vary scales per facet.
     +geom_point()
     #add the points as representations
     +stat_smooth(method='lm', aes(group=1))
     #add the linear fits.
)
```

Note that the means for every subject are at slightly different score levels. There is even more variability in the slopes of the lines. We can capture those with another random effects term for a random slope. 

Random intercept and random slope model:

```{r}
res2 = lmer(score ~ num + (1 + num|subj.id), dl)
summary(res2)
```

Note that we now have more coefficients in the random effects table and our main effects have reduced in significance. 

Is the variance in terms of intercept and slope enough that we need both random terms? We can formally answer this question using `anova` as seen above. 

```{r}
anova(res1, res2)
```

It seems like the model with the random slope does account for significantly more variance! Now you have a research and/or moral dilemma. Do you try to figure out what's causing the variance in slope and intercept? Do you push the simpler but worse model? Or, could there be something else going on?

```{r}
str(dl)
ggplot(dl, aes(x=num, y=score, cond=attn, color=attn)) + 
     facet_wrap(~subj.id, ncol=5, scales='fixed')+
     geom_point()+theme_bw()+
     stat_smooth(method='lm')
```



```{r}
# Set up contrast for attention
contrasts(dl$attn) = c(-1, 1)
contrasts(dl$attn)

res3a = lmer(score ~ scale(num, scale=FALSE) + attn + (1|subj.id), REML=FALSE, dl)
summary(res3a)

res3b = lmer(score ~ scale(num, scale=FALSE) + (1|subj.id), REML=FALSE, dl)
summary(res3b)

anova(res3b, res3a)
```
Including attn in the model significantly improves model fit.

```{r}
res4b = lmer(score ~ scale(num, scale=FALSE) + attn + (1 + num|subj.id), REML=TRUE, dl)
summary(res4b)

coef(res4b)

res4c = lmer(score ~ scale(num, scale=FALSE) + attn + (1 + attn|subj.id), REML=TRUE, dl)
summary(res4c)

coef(res4c)

res4a = lmer(score ~ scale(num, scale=FALSE) + attn + (1|subj.id), REML=TRUE, dl)
summary(res4a)

anova(res4a, res4b)
anova(res4a, res4c)

# res4a is still the best!
summary(res4a)
```

```{r}
res5a = lmer(score ~ scale(num, scale=FALSE) + attn + (1|subj.id), REML=FALSE, dl)

res5b = lmer(score ~ scale(num, scale=FALSE) * attn + (1|subj.id), REML=FALSE, dl)

anova(res5a, res5b)

summary(res5b)
```
The interaction is significant!


```{r}
res6a = lmer(score ~ scale(num, scale=FALSE) * attn + (1|subj.id), REML=TRUE, dl)

res6b = lmer(score ~ scale(num, scale=FALSE) * attn + (1+num|subj.id), REML=TRUE, dl)

res6c = lmer(score ~ scale(num, scale=FALSE) * attn + (1+attn|subj.id), REML=TRUE, dl)

anova(res6a, res6b)
anova(res6a, res6c)

summary(res6a)

# compare to lm
summary(lm(score ~ scale(num, scale=FALSE) * attn, data=dl))
```

```{r}
ggplot(dl, aes(x=num, y=score, cond=attn, color=attn)) + 
  geom_point()+ theme_bw() +
  geom_jitter(position = position_jitter(width = .2)) + 
  stat_smooth(method='lm')
```

There is a significant interaction between number of solutions to the puzzle and attention condition, t = -3.399, such that as the number of solutions to the puzzle decreases (i.e., as the puzzle gets harder) the effect of attention condition on score changes; specifically, when the number of solutions is lowest, divided attention results in a lower score than focused attention. In contrast, when there are more solutions to the puzzle, there is less of a score difference between the divided attention and focused attention conditions.

