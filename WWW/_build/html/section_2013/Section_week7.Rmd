Section Week 7 - Partial correlations, Practice with contrasts, 2x3 ANOVA, Contrasts by hand
========================================================

## Quiz 2 Feedback
----------------------

- Read questions carefully and give an attempted answer for every question

- Manage your time progressing through the quiz

- When testing something: 
1). state the name of the test, 
2). **Give formula or R command**, 
3). Show your work, rather than just writing a number, so that in case you are wrong, we can still give lots of partial credit!

- Calculate statistic and critical value or p value

- Reject the null (or not)

- Informal conclusion

- When writing up results: 
1). use a few numbers, *b = 3.21, t(23) = 2.42, p = 0.012*, 
2). Formal words: There was a significant quadratic effect of smiling on creepiness
3). Informal words: Those who smile more tend to be less creepy, but there is a point at which smiling too much causes an increase in creepiness.


Partial Correlations
----------------------

Remember that both the **Baron & Kenny method** and a test for **partial correlation** can test causal hypotheses.

Let's compare a test with partial correlation to mediation.  Let's quickly review our findings from the **coffee problem** from Homework 3.

**Causal Models**

**IV**: *Coffee* - 20 subjects in each group either had 0 cups, 2 cups, or 4 cups

**DV**: *Performance* - on a stats quiz with 10 problems, 5-89 points

**Possible Mediator 1**: *Number of problems attempted* (hyperactivity)

**Possible Mediator 2**: *Accuracy* - how likely they were to get a problem right if they tried (better success)

```{r coffee data}
d<-read.csv("http://www.stanford.edu/class/psych252/data/caffeine.csv")
str(d)

d$cups = 0*as.numeric(d$coffee==1) + 2*as.numeric(d$coffee==2) + 4*as.numeric(d$coffee==3) 
table(d$cups)
```

### Visualize your data!
```{r}
library(gpairs)
gpairs(d, upper.pars = list(scatter = "lm",
                         conditional = "barcode",
                         mosaic = "mosaic"),
       lower.pars = list(scatter = "stats",
                         conditional = "boxplot",
                         mosaic = "mosaic"),
       stat.pars = list(fontsize = 14, signif = 0.05, 
                        verbose = FALSE, use.color = TRUE, 
                        missing = 'missing', just = 'centre'))
```


### Mediation

Cups of Coffee $\longrightarrow$ Hyperactivity $\longrightarrow$ Performance
Here, "hyperactivity" is measured by the number of problems solved.

What's the model we don't need?:
*Mediator predicting our DV on its own. Just ignore this guy.*
```{r}
with(d, summary(lm(perf~numprob)))
```

```{r mediation models}
with(d, summary(lm(perf~cups))) #c
with(d, summary(lm(numprob~cups))) #a
with(d, summary(lm(perf~cups+numprob))) #b and c'
```


### Partial Correlation
Now let's compare this test of our causal model with a partial correlation test.  Partial correlations find the correlation between two variables after removing the effects of other variables.

x = **IV** *(cups of coffee)*

y = **DV** *(performance)*

z = **third variable** *(mediator)*


First, we'll need to find out what our correlations are.

```{r get corrs}
a = with(d, cor.test(cups,perf)); print(a)
b = with(d, cor.test(cups,numprob)); print(b)
c = with(d, cor.test(numprob,perf)); print(c)
```

What's the formula for a partial correlation?

$$\textrm{Partial Correlation }: r(xy.z) = \frac{{r_{xy} - r_{xz} * r_{yz}}}{\sqrt{(1-r_{xz}^2) * (1-r_{yz}^2)}}$$

```{r partial correlation}
n <- 60
xy<- as.numeric(a$estimate)
xz<- as.numeric(b$estimate)
yz<- as.numeric(c$estimate)

r_xy_z<-(xy-(xz*yz))/(sqrt((1-xz^2)*(1-yz^2))); print(r_xy_z)
t2 <- r_xy_z*sqrt((n-2)/(1-r_xy_z^2)); print(t2)
ptr<-pt(t2,n-2, lower.tail=FALSE); print(ptr) 
```

Now let's try wth accuracy, which we know isn't a mediator! 

Cups of Coffee $\leadsto$ Accuracy $\longrightarrow$ Performance

Quick review of what we found last time:

```{r med mod acc}
with(d, summary(lm(perf~cups))) #c
with(d, summary(lm(accur~cups))) #a
with(d, summary(lm(perf~cups+accur))) #b and c'
```

```{r get corrs acc}
a = with(d, cor.test(cups,perf)); print(a)
b = with(d, cor.test(cups,accur)); print(b)
c = with(d, cor.test(accur,perf)); print(c)
```

```{r pcorr acc}
n <- 60
xy<- as.numeric(a$estimate)
xz<- as.numeric(b$estimate)
yz<- as.numeric(c$estimate)

r_xy_z<-(xy-(xz*yz))/(sqrt((1-xz^2)*(1-yz^2))); print(r_xy_z)
t2 <- r_xy_z*sqrt((n-2)/(1-r_xy_z^2)); print(t2)
ptr<-pt(t2,n-2,lower.tail=FALSE); print(ptr) 
```

### Partial Mediation w/`psych`: `partial.r()`
```{r}
library(psych)
?partial.r
```

For number of problems attempted:
```{r}
da = subset(d,select=c('cups','perf','numprob'))
partial.r(da, 1:2, 3)
```

For accuracy:
```{r}
da1 = subset(d,select=c('cups','perf','accur'))
partial.r(da1, 1:2, 3)
```

So far, we've considered causal models like this:
What if cups of coffee predicted both the number of problems attempted and performance, but performance and the number of problems weren't related? 

So instead of this model:

#### Cups of Coffee $\longrightarrow$ Hyperactivity $\longrightarrow$ Performance

We were interested in this model:

#### Cups of Coffee $\longrightarrow$ Hyperactivity

$\downarrow$

#### Performance

```{r}
n <- 60
xy<- .26
xz<- .32
yz<- .03

r_xy_z<-(xy-(xz*yz))/(sqrt((1-xz^2)*(1-yz^2))); print(r_xy_z)
t2 <- r_xy_z*sqrt((n-2)/(1-r_xy_z^2)); print(t2)
ptr<-pt(t2,n-2, lower.tail=FALSE); print(ptr) 
```


Practice with Contrasts
----------------------

Let's practice interpreting contrasts!

Remember our handy function from last time to test for **orthogonality**? 
```{r}
c_orth_4 <- function(x) {
  a <- (x[1,1]*x[1,2])+(x[2,1]*x[2,2])+(x[3,1]*x[3,2])+(x[4,1]*x[4,2]);
  b <- (x[1,2]*x[1,3])+(x[2,2]*x[2,3])+(x[3,2]*x[3,3])+(x[4,2]*x[4,3]);
  c <- (x[1,1]*x[1,3])+(x[2,1]*x[2,3])+(x[3,1]*x[3,3])+(x[4,1]*x[4,3]);
  d <- cbind(a,b,c); rownames(d)=c('Contrasts'); print(d)
  e <- a+b+c; names(e)=c('Sum'); print(e) }
```

Orthogonal contrasts possible for 4 groups! *Remember that you can have k-1 contrasts, if k is the number of groups that you have*

There are three different sets of orthogonal contrasts that are useful for comparing 4 different groups. These are useful in different situations. Let's consider a series of experiments where we're interested in what predicts whether a grad student is willing to register early for a conference. What do each of these sets of contrasts test?

### When our IV is quantitative:

For instance, let's say that we're looking at level of trait anxiety on likelihood of reigstering early for a conference. 

```{r}
c1 <- cbind(c(-3,-1,1,3), c(1,-1,-1,1), c(-1,3,-3,1)); c_orth_4(c1)
rownames(c1) <- c(1,2,3,4); print(c1)
```

### When we have two levels of one factor, and two levels of another factor.

Let's say that Factor A is fee for late registration (low vs. high)
and Factor B is the number of days before the deadline a warning is issued (one week vs. 1 day)
on likelihood of early registration

```{r}
c2 <- cbind(c(1,1,-1,-1), c(1,-1,0,0), c(0,0,1,-1)); c_orth_4(c2)
rownames(c2) <- c('reg1','reg2','days1','days2'); print(c2)
```

### When we're using a 2x2 factorial design:

Again, let's say that Factor A is gender (male vs. female)
and Factor B is framing of an issue (loss vs. gain)
on likelihood of early registration

```{r}
c3 <- cbind(c(-1,1,-1,1), c(-1,-1,1,1), c(1,-1,-1,1)); c_orth_4(c3)
rownames(c3) <- c('men/loss','women/loss','men/gain','women/gain'); print(c3)
```

Visualizing our design matrix
----------------------

Load in some data
```{r}
d0 = read.csv("http://www.stanford.edu/class/psych252/data/lifesatis.csv")
str(d0)
d0$kids_cat[d0$kids < 1] = 'None'
d0$kids_cat[d0$kids == 1] = 'One'
d0$kids_cat[d0$kids > 1] = 'Multiple'

d0$kids_cat = factor(d0$kids_cat)
str(d0)
```

### Visualize the data
```{r}
library(ggplot2)
ggplot(d0, 
       aes(x=scale(marsatis), 
           y=lifsatis, color=kids_cat)) +
  geom_point(shape=1, position=position_jitter(width=.1)) +  
  geom_smooth(method=lm, se=TRUE) +
  theme_bw()

ggplot(d0, aes(kids_cat, lifsatis, color=kids_cat)) + 
  geom_boxplot()
```            

### View the design matrix

First let's set up some contrasts and see what the `lm()` output looks like:
```{r}
levels(d0$kids_cat) 
contrasts(d0$kids_cat) = cbind(c(1, -2, 1), c(1, 0, -1))

# orthogonal?
apply(contrasts(d0$kids_cat), 2, sum)

colnames(contrasts(d0$kids_cat)) = c('kids-vs-none', 'multiple-vs-one')
contrasts(d0$kids_cat)

lm1 = lm(lifsatis~kids_cat, data=d0); summary(lm1)
```

Now, let's visualize our design matrix:
```{r}
m1 <- model.matrix(lm1); head(m1)

library(gplots) # to use the colorpanel() function

# plot design matrix for intercept + 2 contrasts
image(t(m1), axes=FALSE, frame.plot=TRUE, col=colorpanel(20, "white", "grey10"), main="Design Matrix", ylim=c(1, 0), ylab='Row Number', xlab='Contrast')

# add lifsatis data to matrix
m1 = cbind(m1,as.matrix(d0$lifsatis)); head(m1)

# plot design matrix w/lifsatis data in last column
image(t(m1), axes=FALSE, frame.plot=TRUE, col=colorpanel(20, "white", "grey10"), main="Design Matrix", ylim=c(1, 0), ylab='Row Number')
```


Calculate the significance of a contrast by hand!
---------------------------------------------

First let's get some stats:
```{r}
# some stats
k = 3 # number groups
means = aggregate(lifsatis~kids_cat,data=d0,function(x) mean(x)); means
ns = aggregate(lifsatis~kids_cat,data=d0,function(x) length(x)); ns
N = sum(ns[,2]) # total participants
sems = aggregate(lifsatis~kids_cat, data=d0, function(x) (sd(x) / sqrt(length(x)))); sems
vars = aggregate(lifsatis~kids_cat, data=d0, function(x) (var(x))); vars
grand_mean = mean(d0$lifsatis); grand_mean
mean_ofmeans = mean(means[,2]); mean_ofmeans

# compare intercept to both means!
summary(lm1)
```

Let's test our first contrast (whether having any kids improves life satisfaction).
```{r}
# pull out our contrast, and multiply each contrast by the group means
contrast1 = contrasts(d0$kids_cat)[,1]; contrast1
l1 = contrast1 * means[,2]; l1
l1 = sum(l1); l1

# some more stats
T = sum(ns[,2]*means[,2]); T
C_val = (T^2)/N; C_val
SSj = (ns[,2]-1)*vars[,2]; SSj
SSerror = sum(SSj); SSerror
MSerror = SSerror/(N-k); MSerror

# compare error to anova table from lm output
anova(lm1)

# some extra values..
SSgroup = sum(((ns[,2]*means[,2])^2)/ns[,2]) - C_val; SSgroup
MSgroup = SSgroup/(k-1); MSgroup
Fstat = MSgroup/MSerror; Fstat


# Calculate t-stat for contrast
contrast1^2
(contrast1^2)/ns[,2]

se2 = MSerror * sum((contrast1^2)/ns[,2]); se2

t_stat = l1/sqrt(se2); t_stat
p_val = 2*pt(t_stat, df=N-k, lower.tail=FALSE); p_val
cat("t (", N-k,") = ", t_stat, ", p = ", p_val)

# compare to lm output
summary(lm1)
```